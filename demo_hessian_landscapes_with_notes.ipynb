{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Hessian Loss Landscapes\n",
    "\n",
    "Ashley S. Dale\n",
    "\n",
    "Notebook loads a pretrained ALIGNN model, and calculates the loss landscape using Hessian directions\n",
    "\n",
    "\n",
    "- Relevant paper: [*Visualizing high-dimensional loss landscapes with Hessian directions* by Bottcher and Wheeler](https://iopscience.iop.org/article/10.1088/1742-5468/ad13fc/meta)\n",
    "\n",
    "---\n",
    "Notebook Outline:\n",
    "\n",
    "0. Select and load trained model and data\n",
    "\n",
    "0. Generate a set of predictions for the data\n",
    "\n",
    "0. Select a subset of well predicted instances to be \"In Distribution\" (ID) based on the z-score of the prediction error, where low z-score represents well predicted and therefore in-distribution\n",
    "\n",
    "0. Select a subset of poorly predicted instances to be \"Out of Distribution\" (OOD) based on the z-score of the prediction error, where a high z-score represents poorly predicted and therefore out-of-distribution\n",
    "\n",
    "0. Load Hessian eigenvectors as two new models. These models will define the coordinate axes of the loss landscape\n",
    "\n",
    "0. Calculate the loss landscape using the original model as the origin, and the models generated from the eigenvectors of the Hessian as the two directions in which the original model is perturbed. Repeat this twice for the ID and OOD datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the hessian calculation, these additional packages should be installed\n",
    "# !pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from torchinfo import summary\n",
    "from pymatgen.core.periodic_table import Element\n",
    "from collections import OrderedDict\n",
    "\n",
    "import alignn\n",
    "from alignn.pretrained import *\n",
    "from jarvis.db.figshare import data\n",
    "from jarvis.db.figshare import data\n",
    "from jarvis.db.jsonutils import loadjson\n",
    "\n",
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "from loss_landscapes.model_interface.model_wrapper import ModelWrapper\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch. cuda. is_available():\n",
    "    print(\"Using GPU ...\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pretrained_models = list(get_all_models().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Select the `jv_formation_energy_peratom_alignn` model for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}\n",
    "\n",
    "config_selector = widgets.Dropdown(\n",
    "    options=list_of_pretrained_models,\n",
    "    value=list_of_pretrained_models[0],\n",
    "    description='Select Model',\n",
    "    style=style,\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(config_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the model we will load\n",
    "model_name = config_selector.value\n",
    "print(\"Selected: \", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_figshare_model(model_name)\n",
    "model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wt_dict = OrderedDict([i for i in model.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = 'optb88vdw_bandgap'\n",
    "target = 'formation_energy_peratom'\n",
    "n_samples = 1000\n",
    "element_to_omit_from_training_data = 'Fe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data(\"dft_3d\")\n",
    "d = d[:n_samples]\n",
    "dataset = DataFrame(copy.deepcopy(d))\n",
    "atoms_df = DataFrame(list(DataFrame(d)['atoms']))\n",
    "dataset = pd.concat([dataset, atoms_df], axis=1)\n",
    "train_idx, test_idx = get_split(dataset, 'elements', element_to_omit_from_training_data)\n",
    "print('num train samples: '+ str(len(train_idx)))\n",
    "print('num test samples: '+ str(len(test_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on Test and Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split all samples based having or not having Fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [d[idx] for idx in train_idx.to_list()]\n",
    "train_dataloader = get_data_loader(train_data, target, workers=0)\n",
    "\n",
    "model_train_predictions = []\n",
    "original_train_targets = []\n",
    "for s in tqdm(train_dataloader):\n",
    "    original_train_targets.append(s[2].detach().numpy()[0])\n",
    "    y_pred = model([s[0].to(device), s[1].to(device)])\n",
    "    y_pred = np.expand_dims(y_pred.cpu().detach().numpy(), axis=0)[0]\n",
    "    model_train_predictions.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [d[idx] for idx in test_idx.to_list()]\n",
    "test_dataloader = get_data_loader(test_data, target, workers=0)\n",
    "\n",
    "model_test_predictions = []\n",
    "original_test_targets = []\n",
    "for s in tqdm(test_dataloader):\n",
    "    original_test_targets.append(s[2].detach().numpy()[0])\n",
    "    y_pred = model([s[0].to(device), s[1].to(device)])\n",
    "    y_pred = np.expand_dims(y_pred.cpu().detach().numpy(), axis=0)[0]\n",
    "    model_test_predictions.append(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subselect Train Data Samples: Most ID (Minimum Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample_in_loader = 50 #choose the number of samples involved in hessian computation and loss landscape visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data)\n",
    "train_df['pred_val'] = model_train_predictions\n",
    "train_df['err'] = (train_df[target] - train_df['pred_val'])\n",
    "train_df['abs_err'] = np.abs(train_df[target] - train_df['pred_val'])\n",
    "train_df['z_score_err'] = (train_df['abs_err'] - np.mean(train_df['abs_err']))/np.std(train_df['abs_err'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.hist(train_df['z_score_err'].values, bins=100)\n",
    "plt.title('Train Set Predictions (Omitting '+element_to_omit_from_training_data+')')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select samples with minimal error\n",
    "train_subset_df = train_df.nsmallest(num_sample_in_loader, 'z_score_err')\n",
    "train_subset_df_idx = train_subset_df.index.values.tolist()\n",
    "train_subset_list = [train_data[i] for i in train_subset_df_idx]\n",
    "train_subset_dataloader = get_data_loader(train_subset_list, target, workers=4)\n",
    "train_subset_df.head() #contains the ids of train sub-selected samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train subset to dataloader\n",
    "subset_train_x = []\n",
    "subset_train_y = []\n",
    "\n",
    "for i in train_subset_dataloader:\n",
    "    subset_train_x.append((i[0], i[1]))\n",
    "    subset_train_y.append(i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subselect Test Samples - Most OOD (Maximum Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_data)\n",
    "test_df['pred_val'] = model_test_predictions\n",
    "test_df['err'] = (test_df[target] - test_df['pred_val'])\n",
    "test_df['abs_err'] = np.abs(test_df[target] - test_df['pred_val'])\n",
    "test_df['z_score_err'] = (test_df['abs_err'] - np.mean(train_df['abs_err']))/np.std(train_df['abs_err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset_df = test_df.nlargest(num_sample_in_loader, 'z_score_err')\n",
    "test_subset_df_idx = test_subset_df.index.values.tolist()\n",
    "test_subset_list = [test_data[i] for i in test_subset_df_idx]\n",
    "test_subset_dataloader = get_data_loader(test_subset_list, target, workers=4)\n",
    "test_subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.hist(test_df['z_score_err'].values, bins=100,)\n",
    "plt.title('Test Set Predictions (Exclusively '+element_to_omit_from_training_data+')')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Formatting Eigenvectors from saved local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
    "    \n",
    "\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(torch.nn.MSELoss()(y_pred, y_true))  # Compute RMSE from MSE\n",
    "\n",
    "LogCoshLoss()\n",
    "\n",
    "RMSELoss()\n",
    "\n",
    "torch.nn.L1Loss()\n",
    "\n",
    "torch.nn.MSELoss()\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defaults from OG implementation\n",
    "\n",
    "func = copy.deepcopy(model)\n",
    "og_params = [i[1] for i in func.named_parameters() if len(i[1].size()) > 1]\n",
    "og_layer_names = [i[0] for i in func.named_parameters() if len(i[1].size())>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eig_max = copy.deepcopy(func)\n",
    "model_eig_max.load_state_dict(torch.load('model_eig_max.pt', weights_only=True))\n",
    "model_eig_min = copy.deepcopy(func)\n",
    "model_eig_min.load_state_dict(torch.load('model_eig_min.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 2D Directed Loss Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "\n",
    "from loss_landscapes.model_interface.model_wrapper import ModelWrapper\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specifie resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the custom model wrapper for the loss landscapes calculation\n",
    "class Metric(ABC):\n",
    "    \"\"\" A quantity that can be computed given a model or an agent. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, model_wrapper: ModelWrapper):\n",
    "        pass\n",
    "\n",
    "class Loss(Metric):\n",
    "    \"\"\" Computes a specified loss function over specified input-output pairs. \"\"\"\n",
    "    def __init__(self, loss_fn, model, inputs: torch.Tensor, target: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.loss_fn = loss_fn\n",
    "        self.inputs = inputs\n",
    "        self.model = model\n",
    "        self.target = target\n",
    "\n",
    "    def __call__(self, model_wrapper: ModelWrapper) -> float:\n",
    "        outputs = model_wrapper.forward(self.inputs)\n",
    "        err = self.loss_fn(self.target[0], outputs)\n",
    "        return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each sample in dataloader, compute losslandscape, and saves to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: \n",
    "- test on two versions of perturbation:\n",
    "1. Direction = [eigenvector-original_weight], which is the original implementation\n",
    "2. Direction = [eigenvector], need to adjust the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_landscapes_list = []\n",
    "for i, batch in tqdm(enumerate(train_subset_dataloader), total=len(train_subset_dataloader)):\n",
    "    x_train = (batch[0].to(device), batch[1].to(device))\n",
    "    y_train = (batch[2].to(device))\n",
    "    metric = Loss(loss_func, func.eval(), x_train, y_train)\n",
    "\n",
    "    # print('True', float(y_train))\n",
    "    # print('Predicted',float(model(x_train)))\n",
    "    # print('MSE',np.abs((float(y_train)-float(model(x_train))))**2)\n",
    "    # print('metric', metric(func.eval()))\n",
    "\n",
    "    try:\n",
    "        loss_data_fin = loss_landscapes.planar_interpolation_v4( #use updated version of planar interpolation\n",
    "            model_start=func.eval(), \n",
    "            model_end_one=model_eig_max.eval(),\n",
    "            model_end_two=model_eig_min.eval(),\n",
    "            metric=metric, steps=STEPS, deepcopy_model=True\n",
    "            )\n",
    "        ## TODO: ugly patch to convert array from torch.tensors to np.array\n",
    "        tmp_ll = []\n",
    "        for row in loss_data_fin:\n",
    "            tmp_row = []\n",
    "            for itm in row:\n",
    "                tmp_row.append(itm.detach().cpu().numpy())\n",
    "            tmp_ll.append(tmp_row)\n",
    "    except Exception as e:\n",
    "        print(e+'batch id: '+str(i))\n",
    "        # continue\n",
    "    train_loss_landscapes_list.append(np.expand_dims(np.array(tmp_ll), axis=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computes average and standard deviation of the loss landscape array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.concatenate(train_loss_landscapes_list, axis=2) \n",
    "avg_loss_landscape = np.mean(tmp, axis=2) #averaging the loss landscape for all samples\n",
    "std_loss_landscape = np.std(tmp, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig_name = os.path.join('loss_contours_train.png')\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.imshow(np.log10(avg_loss_landscape), cmap='jet', origin='lower')\n",
    "ax.set_title('Averaged train sample Loss Contours in log scale \\n'+ r'$L(\\theta + \\alpha i + \\beta j$)')\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "ax.set_ylabel(r'$\\beta$')\n",
    "plt.colorbar()\n",
    "#fig.savefig(save_fig_name, transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_landscapes_list = []\n",
    "for i, batch in tqdm(enumerate(test_subset_dataloader), total=len(test_subset_dataloader)):\n",
    "    x_train = (batch[0].to(device), batch[1].to(device))\n",
    "    y_train = (batch[2].to(device))\n",
    "    metric = Loss(loss_func, func.eval(), x_train, y_train) #actually x_test,y_test but named differently\n",
    "    try:\n",
    "        loss_data_fin = loss_landscapes.planar_interpolation_v4(\n",
    "            model_start=func.eval(), \n",
    "            model_end_one=model_eig_max.eval(),\n",
    "            model_end_two=model_eig_min.eval(),\n",
    "            metric=metric, steps=STEPS, deepcopy_model=True\n",
    "            )\n",
    "        ## TODO: ugly patch to convert array from torch.tensors to np.array\n",
    "        tmp_ll = []\n",
    "        for row in loss_data_fin:\n",
    "            tmp_row = []\n",
    "            for itm in row:\n",
    "                tmp_row.append(itm.detach().cpu().numpy())\n",
    "            tmp_ll.append(tmp_row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e+'batch id: '+str(i)) \n",
    "        # continue\n",
    "    test_loss_landscapes_list.append(np.expand_dims(np.array(tmp_ll), axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computes average and standard deviation of the loss landscape array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.concatenate(test_loss_landscapes_list, axis=2)\n",
    "test_avg_loss_landscape = np.mean(tmp, axis=2)\n",
    "test_std_loss_landscape = np.std(tmp, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig_name = os.path.join('loss_contours_test.png')\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.imshow(np.log10(test_avg_loss_landscape), cmap='jet', origin='lower')\n",
    "ax.set_title('Averaged test sample Loss Contours in log scale \\n'+ r'$L(\\theta + \\alpha i + \\beta j$)')\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "ax.set_ylabel(r'$\\beta$')\n",
    "plt.colorbar()\n",
    "#fig.savefig(save_fig_name, transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If STEPS, loss function, or training subset are changed, then start in a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "def initialize_save(output_data, base_path,comment):\n",
    "    # Create a timestamp for the folder name\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    folder_name = f\"output_{timestamp}_\"+comment\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Created folder: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Folder already exists: {folder_path}\")\n",
    "\n",
    "    # Create a text file in the folder\n",
    "    file_name = \"parameters.txt\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Save the output data to the text file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(output_data)\n",
    "    \n",
    "    print(f\"Output saved to: {file_path}\")\n",
    "\n",
    "    return folder_path\n",
    "\n",
    "def save_dfs_to_csv(train_df, test_df, base_path):\n",
    "    # Create a timestamp for the folder name\n",
    "    folder_path = base_path\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    train_csv_path = os.path.join(folder_path, \"train_subset.csv\")\n",
    "    test_csv_path = os.path.join(folder_path, \"test_subset.csv\")\n",
    "\n",
    "    train_df.to_csv(train_csv_path, index=False)\n",
    "    test_df.to_csv(test_csv_path, index=False)\n",
    "\n",
    "    print(f\"Train subset saved to: {train_csv_path}\")\n",
    "    print(f\"Test subset saved to: {test_csv_path}\")\n",
    "# Example usage\n",
    "output_data = (\n",
    "    f'Model name = {model_name}\\n'\n",
    "    f'STEPS = {STEPS}\\n'\n",
    "    f'Loss function = {loss_func}\\n'\n",
    "    f'# Train subset = {len(train_subset_df)}\\n'\n",
    "    f'# Test subset = {len(test_subset_df)}'\n",
    ")\n",
    "base_path = r\"C:\\Users\\EthanH24\\Desktop\\ML research\\loss_landscapes_demo\\outputs\"  # Specify your base path here\n",
    "\n",
    "# new_folder_path = initialize_save(output_data, base_path,'dir=eigenvec-starting_point,corrected_steps') #!!!!!!!!!!!!!this is new folder path\n",
    "\n",
    "# save_dfs_to_csv(train_subset_df, test_subset_df, new_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for train loss landscape in train loss landscape list:\n",
    "    index the training sample in train subset df\n",
    "    create a folder named df['jid']\n",
    "\n",
    "    save the training sample in train subset df in txt\n",
    "\n",
    "    save train loss landscape in txt\n",
    "    save train loss landscape png\n",
    "    save the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_landscapes(train_or_test, loss_landscape_list, train_subset_df, folder_path):\n",
    "\n",
    "    for i, loss_landscape in enumerate(loss_landscape_list):\n",
    "        # Index the training sample in train_subset_df\n",
    "        sample_index = i  # Assuming the index corresponds to the loss landscape\n",
    "        sample_data = train_subset_df.iloc[sample_index]\n",
    "\n",
    "        # Create a folder named df['jid']\n",
    "        jid_folder = os.path.join(folder_path, train_or_test+str(sample_data['jid']))\n",
    "        if not os.path.exists(jid_folder):\n",
    "            os.makedirs(jid_folder)\n",
    "            print(f\"Created folder: {jid_folder}\")\n",
    "\n",
    "         # Save the training sample in train_subset_df as a CSV file\n",
    "        sample_csv_path = os.path.join(jid_folder, train_or_test+str(sample_data['jid'])+\"sample_data.csv\")\n",
    "        sample_data.to_frame().T.to_csv(sample_csv_path, index=False)  # Convert Series to DataFrame and save\n",
    "\n",
    "\n",
    "\n",
    "         # Save train loss landscape in a text file\n",
    "        loss_array_path = os.path.join(jid_folder, train_or_test+str(sample_data['jid'])+\"_loss_landscape_array.npy\")\n",
    "\n",
    "        np.save(loss_array_path, loss_landscape)\n",
    "\n",
    "\n",
    "        save_fig_name = os.path.join(jid_folder,train_or_test+str(sample_data['jid'])+'_loss_contours.png')\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        plt.imshow(np.log10(loss_landscape), cmap='jet', origin='lower')\n",
    "        ax.set_title(str(sample_data['jid'])+ 'Sample Loss Contours in log scale \\n'+ r'$L(\\theta + \\alpha i + \\beta j$)')\n",
    "        ax.set_xlabel(r'$\\alpha$')\n",
    "        ax.set_ylabel(r'$\\beta$')\n",
    "        plt.colorbar()\n",
    "        fig.savefig(save_fig_name, transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        save_fig_name = os.path.join(folder_path,train_or_test+str(sample_data['jid'])+'_loss_contours.png')\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        plt.imshow(np.log10(loss_landscape), cmap='jet', origin='lower')\n",
    "        ax.set_title(str(sample_data['jid'])+ 'Sample Loss Contours in log scale \\n'+ r'$L(\\theta + \\alpha i + \\beta j$)')\n",
    "        ax.set_xlabel(r'$\\alpha$')\n",
    "        ax.set_ylabel(r'$\\beta$')\n",
    "        plt.colorbar()\n",
    "        fig.savefig(save_fig_name, transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Save the output data (if needed)\n",
    "        output_txt_path = os.path.join(jid_folder, train_or_test+str(sample_data['jid'])+\"_calculation parameters.txt\")\n",
    "        with open(output_txt_path, 'w') as f:\n",
    "            f.write(output_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folder_path = initialize_save(output_data, base_path,'MSE_centered') #!!!!!!!!!!!!!this is new folder path\n",
    "\n",
    "save_dfs_to_csv(train_subset_df, test_subset_df, new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "folder_path = new_folder_path\n",
    "save_loss_landscapes('train_', train_loss_landscapes_list, train_subset_df, folder_path)\n",
    "\n",
    "save_loss_landscapes('test_', test_loss_landscapes_list, test_subset_df, folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "losslandscapefeb8gpu4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
